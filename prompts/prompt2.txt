The following is the abstract of the paper called "Training Very Deep Networks":

"""
Theoretical and empirical evidence indicates that the depth of neural networks
is crucial for their success. However, training becomes more difficult as depth
increases, and training of very deep networks remains an open problem. Here we
introduce a new architecture designed to overcome this. Our so-called highway
networks allow unimpeded information flow across many layers on information
highways. They are inspired by Long Short-Term Memory recurrent networks and
use adaptive gating units to regulate the information flow. Even with hundreds of
layers, highway networks can be trained directly through simple gradient descent.
This enables the study of extremely deep and efficient architectures.
"""

The following is the abstract of a different paper called "Pixel Recurrent Neural Networks":

"""
Modeling the distribution of natural images is
a landmark problem in unsupervised learning.
This task requires an image model that is at
once expressive, tractable and scalable. We
present a deep neural network that sequentially
predicts the pixels in an image along the two
spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the
image. Architectural novelties include fast twodimensional recurrent layers and an effective use
of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the
previous state of the art. Our main results also
provide benchmarks on the diverse ImageNet
dataset. Samples generated from the model appear crisp, varied and globally coherent.
"""

The following is a detailed explanation of how are both papers related to the topic "recurrency":